{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8be4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd29395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cab2efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = list('abcdefghijklmnopqrstuvwxyz')\n",
    "stoi = { s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = { i+1:s for i, s in enumerate(chars)}\n",
    "itos[0] = '.'\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbf4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)    \n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838f263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57479e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# C has shape 27 * 10\n",
    "# C[X[ix]]\n",
    "# X[ix] has shape 32 * 10\n",
    " \n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5 / 3) * (1 / ((n_embd * block_size)**0.5))\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden))* 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ecf375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb89531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8279, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb] # 32 * 3 * 10\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1\n",
    "bnmeani = (1 / n) * (hprebn.sum(0, keepdim = True))\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff ** 2\n",
    "bnvar = (1 / (n - 1)) * (bndiff2.sum(0, keepdim = True))\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# cross entropy loss: max log likelihood, min negative log likelyhood\n",
    "logit_maxes = logits.max(1, keepdim=True).values \n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim = True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log() # 27 * 32\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts_sum, counts_sum_inv, counts, norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, bnmeani, hprebn, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n",
    "# B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f24728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnbias         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbngain         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnraw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnvar_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnvar          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbndiff2        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbndiff         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnmeani        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhprebn         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dembcat         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "demb            | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dC              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Backprob thrugh the whole thing manually\n",
    "\n",
    "dloss = 1.0\n",
    "dlogprobs = torch.zeros((logprobs.shape[0], logprobs.shape[1]))\n",
    "for i in range(n):\n",
    "    dlogprobs[i][Yb[i]] =  -1.0 / batch_size\n",
    "# print(dlogprobs[0])\n",
    "# print(logprobs.grad[0])\n",
    "# cmp('dlogprobs', dlogprobs, logprobs)\n",
    "\n",
    "# dprobs / dloss = (dprobs / dlogprops) * (dlogprops / dloss)\n",
    "# dprobs has shape same a probs i.e. 32 * 27\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "# print(dprobs[0])\n",
    "# print(probs.grad[0])\n",
    "cmp('dprobs', dprobs, probs)\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # shape: 32 * 1\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "\n",
    "dcounts_sum =  (-1.0 / (counts_sum)**2) * dcounts_sum_inv # has shape 32 x 1\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "cmp('dcounts', dcounts, counts)\n",
    "\n",
    "# dcounts = (counts_sum_inv + counts * (-1.0 / (counts_sum ** 2))) # * dprobs\n",
    "# print(dcounts[1])\n",
    "# print(counts.grad[1])\n",
    "# cmp('dcounts', dcounts, counts)\n",
    "\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "dlogit_maxes = (-1.0 * dnorm_logits).sum(1, keepdim=True) # only for logits which were max shape: 32 * 1\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "dlogits = dnorm_logits\n",
    "for i in range(logits.shape[0]):\n",
    "    for j in range(logits.shape[1]):\n",
    "        if logits[i][j] == logit_maxes[i]:\n",
    "            dlogits[i][j] += dlogit_maxes[i][0]\n",
    "cmp('dlogits', dlogits, logits)\n",
    "\n",
    "db2 = dlogits.sum(0, keepdim=True)\n",
    "cmp('db2', db2, b2)\n",
    "\n",
    "\n",
    "# h has shape 32 * 200\n",
    "# W2 has shape 200 * 27\n",
    "# dlogits has shape 32 * 27\n",
    "# print(W2.shape)\n",
    "# print(dlogits.shape)\n",
    "dh = torch.transpose(W2 @ torch.transpose(dlogits, 0, 1), 0, 1)\n",
    "cmp('dh', dh, h)\n",
    "\n",
    "dW2 = torch.transpose(h, 0, 1) @ dlogits\n",
    "cmp('dW2', dW2, W2)\n",
    "\n",
    "\n",
    "dhpreact = (1 - h**2) * dh\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "\n",
    "# emb = C[Xb] # 32 * 3 * 10\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "# # Linear layer 1\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# bnmeani = (1 / n) * (hprebn.sum(0, keepdim = True))\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff ** 2\n",
    "# bnvar = (1 / (n - 1)) * (bndiff2.sum(0, keepdim = True))\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdim = True)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim = True)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "\n",
    "dbnraw = bngain * dhpreact\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim = True)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "# bnvar has shape 1 * 200\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "\n",
    "# bnvar = (1 / (n - 1)) * (bndiff2.sum(0, keepdim = True))\n",
    "# bndiff2 has shape 32 * 200\n",
    "dbndiff2 = torch.ones_like(bndiff2) * (1 / (n - 1)) * dbnvar\n",
    "# print(bndiff2.shape)\n",
    "# print(dbndiff2.shape)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "dbndiff = 2 * bndiff * dbndiff2\n",
    "dbndiff += bnvar_inv * dbnraw\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "\n",
    "# bndiff has shape 32 * 200\n",
    "# bnmeani has shape 1 * 200\n",
    "dbnmeani = (-1.0 * dbndiff).sum(0, keepdim = True) \n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "# emb = C[Xb] # 32 * 3 * 10\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# bnmeani = (1 / n) * (hprebn.sum(0, keepdim = True))\n",
    "# bndiff = hprebn - bnmeani\n",
    "\n",
    "# hprebn has shape 32 * 200\n",
    "dhprebn = dbndiff\n",
    "# bnmeani has shape 1 * 200\n",
    "dhprebn += (1 / n) * dbnmeani\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "\n",
    "db1 = dhprebn.sum(0, keepdim = True)\n",
    "cmp('db1', db1, b1)\n",
    "\n",
    "dembcat = dhprebn @ torch.transpose(W1, 0, 1)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "\n",
    "dW1 = torch.transpose(embcat, 0, 1) @ dhprebn\n",
    "cmp('dW1', dW1, W1)\n",
    "\n",
    "demb = dembcat.view(emb.shape[0], emb.shape[1], -1)\n",
    "cmp('demb', demb, emb)\n",
    "\n",
    "# emb = C[Xb]\n",
    "# Xb has shape 32 * 3\n",
    "# C has shape 27 * 10\n",
    "# emb has shape 32 * 3 * 10\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        dC[Xb[i][j]] += demb[i][j]\n",
    "cmp('dC', dC, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ea10d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = torch.zeros_like(logits)\n",
    "foo[range(logits.shape[0]), Yb] = 1 / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "193ed9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1675],\n",
      "        [ 0.6853],\n",
      "        [11.1944],\n",
      "        [-4.7283],\n",
      "        [ 3.7751],\n",
      "        [ 2.2485],\n",
      "        [-4.2482],\n",
      "        [-8.2952],\n",
      "        [ 3.9967],\n",
      "        [ 7.8760],\n",
      "        [-5.9808],\n",
      "        [-0.3015],\n",
      "        [ 4.5658],\n",
      "        [-6.4188],\n",
      "        [ 1.1241],\n",
      "        [ 6.4003],\n",
      "        [ 8.2901],\n",
      "        [ 6.5472],\n",
      "        [ 3.4046],\n",
      "        [-8.2952],\n",
      "        [ 9.7022],\n",
      "        [-6.9753],\n",
      "        [ 2.8430],\n",
      "        [ 1.8309],\n",
      "        [ 3.0741],\n",
      "        [-6.2279],\n",
      "        [ 1.8356],\n",
      "        [-0.4410],\n",
      "        [-0.8160],\n",
      "        [-8.2952],\n",
      "        [-4.2253],\n",
      "        [-8.2952]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "bar = logits.sum(1, keepdim = True) - logits + (1.0 - vocab_size) * logits.max(1, keepdim = True).values\n",
    "bar *= foo\n",
    "# print(bar)\n",
    "A = logits.sum(1, keepdim = True)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f77b2040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: False | approximate: False | maxdiff: 2.1872243881225586\n"
     ]
    }
   ],
   "source": [
    "cmp('dlogits', bar, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35076913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5640e-04, -2.4939e-02,  5.1969e-04,  1.0260e-03,  1.8752e-03,\n",
       "          5.2618e-04,  3.0502e-04,  4.0244e-04,  1.0106e-03,  6.9478e-04,\n",
       "          1.8658e-03,  2.1820e-04,  6.5081e-04,  1.6950e-04,  6.3564e-04,\n",
       "          6.1022e-04,  7.7567e-04,  1.7753e-04,  6.4878e-05,  5.0351e-04,\n",
       "          4.4877e-03,  1.3322e-03,  2.3320e-03,  4.2050e-04,  1.2719e-04,\n",
       "          1.8821e-03,  1.9692e-03],\n",
       "        [ 1.6959e-03,  1.2657e-03,  7.0448e-04,  1.1192e-03,  1.0531e-03,\n",
       "          3.9933e-04,  2.5606e-04,  7.6596e-04,  2.4770e-03,  1.1053e-03,\n",
       "          7.0484e-04,  1.4427e-04, -3.0693e-02,  1.7573e-03,  2.8811e-04,\n",
       "          1.2277e-03,  6.5333e-04,  7.4730e-04,  2.7627e-03,  1.6487e-03,\n",
       "          1.3067e-03,  5.1192e-04,  1.3522e-03,  1.7902e-03,  1.8281e-03,\n",
       "          2.0353e-04,  2.9241e-03],\n",
       "        [-3.0514e-02,  1.0445e-03,  2.6628e-03,  4.6614e-04,  7.1869e-04,\n",
       "          8.5898e-04,  1.3593e-03,  1.1446e-03,  1.4555e-03,  2.3716e-03,\n",
       "          1.9871e-03,  3.0276e-04,  4.1185e-04,  3.3654e-04,  3.0273e-03,\n",
       "          4.3974e-03,  2.3559e-04,  1.5373e-03,  1.9241e-03,  1.2481e-04,\n",
       "          8.2189e-04,  4.4881e-04,  4.6018e-04,  1.0103e-04,  1.0205e-03,\n",
       "          7.6625e-04,  5.2829e-04],\n",
       "        [ 5.8173e-04,  2.8320e-03,  2.0456e-03,  3.0513e-03,  6.0214e-04,\n",
       "         -3.0421e-02,  6.4926e-04,  1.0581e-03,  9.3436e-04,  2.5250e-04,\n",
       "          1.6948e-03,  1.6886e-03,  7.3796e-04,  6.6344e-04,  3.3711e-04,\n",
       "          1.3043e-03,  1.2049e-03,  2.5654e-04,  9.6250e-04,  3.8477e-04,\n",
       "          1.2638e-03,  4.3939e-04,  7.5640e-04,  5.6531e-04,  2.7723e-04,\n",
       "          3.5215e-03,  2.3557e-03],\n",
       "        [ 9.4413e-04,  9.7300e-04,  2.3991e-03,  4.7506e-04,  6.5274e-04,\n",
       "          2.0680e-04,  4.3715e-03,  1.1609e-03,  1.8697e-03, -3.0106e-02,\n",
       "          2.1069e-03,  7.4867e-04,  5.8343e-04,  1.1046e-03,  4.4017e-04,\n",
       "          1.1793e-03,  3.4141e-04,  1.0474e-03,  3.5744e-03,  4.3425e-04,\n",
       "          1.2648e-03,  2.2007e-04,  4.2076e-04,  8.8966e-04,  1.1115e-03,\n",
       "          4.8799e-04,  1.0979e-03],\n",
       "        [ 1.1525e-04,  3.8801e-03,  6.8145e-04,  6.2878e-04,  1.0946e-03,\n",
       "          2.3867e-03,  1.3814e-03,  1.3639e-03,  1.3034e-04,  3.9213e-04,\n",
       "          6.1252e-04,  4.5124e-03,  7.0858e-04,  3.5137e-04,  1.4274e-03,\n",
       "          3.1695e-04,  1.1605e-03,  7.1838e-04, -3.1001e-02,  3.5988e-04,\n",
       "          1.0795e-03,  3.1540e-03,  3.8427e-04,  5.8505e-04,  1.6119e-03,\n",
       "          1.0346e-03,  9.2870e-04],\n",
       "        [ 8.4901e-04,  1.3983e-03,  8.3913e-04,  6.2904e-04,  5.8201e-04,\n",
       "          1.1107e-03,  9.6339e-04,  3.0002e-04,  5.0472e-04, -3.1062e-02,\n",
       "          1.4310e-04,  2.9819e-03,  2.3173e-03,  1.1158e-03,  5.1204e-03,\n",
       "          2.1373e-04,  4.2383e-04,  2.5411e-03,  4.8616e-04,  2.7757e-04,\n",
       "          4.6570e-04,  3.2944e-04,  1.2928e-03,  1.8149e-03,  5.3408e-04,\n",
       "          8.0951e-04,  3.0183e-03],\n",
       "        [ 2.6016e-03,  7.8779e-04,  2.1745e-03,  2.0392e-03,  2.6551e-04,\n",
       "          3.2451e-04,  3.9727e-04,  1.4619e-03,  2.7503e-03,  3.0624e-04,\n",
       "          4.4896e-04,  2.4191e-03,  6.3386e-04,  6.4013e-04,  1.6419e-03,\n",
       "          6.5823e-04, -3.0618e-02,  2.4241e-03,  1.4949e-03,  4.6209e-04,\n",
       "          9.0540e-04,  1.5449e-03,  8.9684e-04,  1.0490e-03,  7.0242e-04,\n",
       "          8.3859e-04,  7.4829e-04],\n",
       "        [ 1.0579e-03, -3.0472e-02,  4.6822e-04,  1.8988e-03,  5.4517e-04,\n",
       "          3.1358e-04,  1.3547e-03,  2.2700e-03,  8.1388e-04,  7.8276e-04,\n",
       "          9.4122e-04,  6.7457e-04,  1.9934e-03,  1.7761e-03,  7.5338e-04,\n",
       "          2.2176e-03,  2.2439e-03,  7.0458e-04,  3.7146e-04,  9.9930e-04,\n",
       "          1.3398e-03,  1.0638e-03,  3.0366e-03,  1.2898e-03,  8.4839e-04,\n",
       "          5.4475e-04,  1.6841e-04],\n",
       "        [-3.0976e-02,  3.5021e-03,  1.8895e-04,  3.3642e-04,  4.5410e-03,\n",
       "          2.3101e-03,  1.1310e-03,  1.3207e-03,  6.6995e-04,  4.7062e-04,\n",
       "          1.9888e-03,  3.9675e-04,  5.7188e-04,  7.2291e-04,  1.4222e-03,\n",
       "          6.0113e-04,  2.4068e-04,  6.1241e-04,  4.9376e-04,  3.9862e-04,\n",
       "          1.2553e-03,  2.5567e-03,  1.8836e-03,  1.5171e-04,  1.2790e-03,\n",
       "          6.6367e-04,  1.2656e-03],\n",
       "        [-3.0538e-02,  6.5836e-04,  2.1911e-04,  9.2201e-04,  3.4397e-04,\n",
       "          4.1338e-03,  6.2750e-05,  4.4938e-04,  4.6339e-03,  2.3892e-04,\n",
       "          1.3534e-04,  9.0085e-04,  8.9683e-04,  2.1740e-04,  8.2280e-03,\n",
       "          1.6533e-04,  1.8639e-04,  7.7246e-04,  1.1235e-04,  3.6066e-04,\n",
       "          2.0245e-04,  1.7143e-03,  2.4974e-03,  6.8948e-04,  6.5146e-04,\n",
       "          1.0104e-03,  1.3516e-04],\n",
       "        [ 1.5491e-03, -3.0839e-02,  1.0065e-03,  1.4309e-03,  2.0591e-03,\n",
       "          4.5847e-04,  4.8314e-04,  1.4918e-03,  1.4607e-03,  9.3816e-04,\n",
       "          6.2185e-04,  1.3914e-03,  5.5523e-04,  4.1673e-04,  8.1494e-04,\n",
       "          1.8233e-03,  7.0237e-04,  9.9980e-04,  8.6399e-04,  1.9344e-03,\n",
       "          3.2885e-04,  3.2487e-04,  2.9749e-04,  2.6491e-04,  6.2341e-03,\n",
       "          3.2308e-04,  2.0634e-03],\n",
       "        [-3.0941e-02,  3.8904e-04,  1.7459e-03,  5.3105e-05,  2.7399e-03,\n",
       "          5.8102e-03,  2.3431e-03,  1.4678e-03,  3.2304e-04,  1.7373e-03,\n",
       "          1.6725e-03,  5.0865e-04,  5.3713e-04,  4.8374e-04,  5.4167e-04,\n",
       "          6.3690e-04,  3.8590e-04,  9.5664e-04,  2.9731e-03,  6.6678e-04,\n",
       "          1.5437e-03,  5.8098e-04,  2.9583e-04,  2.5207e-04,  3.3588e-04,\n",
       "          1.4373e-03,  5.2279e-04],\n",
       "        [-3.0290e-02,  2.5261e-04,  6.9338e-04,  8.0973e-04,  8.0255e-04,\n",
       "          6.3419e-03,  2.2834e-04,  2.0182e-04,  4.0560e-03,  3.4508e-04,\n",
       "          2.6974e-04,  6.1399e-04,  1.4340e-03,  6.0952e-04,  1.7124e-03,\n",
       "          1.9039e-04,  4.3935e-04,  1.0572e-03,  9.3584e-05,  1.1093e-03,\n",
       "          1.0181e-03,  7.6056e-04,  3.6978e-03,  1.9640e-03,  7.7752e-04,\n",
       "          6.5653e-04,  1.5501e-04],\n",
       "        [ 2.4470e-04,  8.2473e-04,  2.3398e-04,  7.3521e-04,  2.6768e-03,\n",
       "          4.2580e-04,  1.9699e-04,  3.7294e-03,  3.0262e-04,  5.3257e-04,\n",
       "          4.0589e-04,  4.8115e-04,  3.0576e-04,  1.2315e-03, -3.1032e-02,\n",
       "          2.5005e-03,  1.5636e-03,  1.5578e-04,  1.5295e-03,  4.2874e-03,\n",
       "          5.1109e-04,  2.7432e-04,  6.9193e-04,  1.0480e-03,  5.2281e-03,\n",
       "          3.3707e-04,  5.7732e-04],\n",
       "        [ 7.6888e-04,  2.7040e-04,  1.9148e-03,  2.5354e-04,  1.3928e-03,\n",
       "          7.6391e-04,  5.3256e-04,  5.5014e-04,  4.0864e-04,  6.3862e-03,\n",
       "          2.3511e-03,  9.1639e-05, -3.0406e-02,  1.0882e-03,  4.6951e-04,\n",
       "          1.4919e-03,  2.5053e-04,  5.8370e-04,  1.0982e-03,  2.2181e-03,\n",
       "          4.2772e-04,  2.4167e-04,  3.1572e-04,  6.8190e-04,  1.5664e-03,\n",
       "          2.8331e-04,  4.0049e-03],\n",
       "        [-3.1070e-02,  7.0474e-04,  5.7623e-04,  1.2018e-03,  1.9110e-03,\n",
       "          1.1155e-03,  2.5594e-04,  1.1126e-03,  3.6984e-04,  6.9346e-04,\n",
       "          4.7665e-03,  5.5514e-04,  1.9570e-03,  4.7981e-04,  1.8630e-03,\n",
       "          1.6635e-03,  5.8957e-04,  2.4799e-04,  1.5084e-04,  1.4328e-03,\n",
       "          3.6503e-04,  7.6929e-04,  1.7710e-03,  5.9047e-04,  3.5264e-03,\n",
       "          4.4048e-04,  1.9605e-03],\n",
       "        [-3.0562e-02,  5.6405e-04,  1.1646e-03,  1.6710e-04,  3.7196e-03,\n",
       "          8.6041e-03,  1.7347e-03,  2.9653e-04,  8.7114e-04,  7.3711e-04,\n",
       "          1.0604e-03,  1.9341e-04,  1.6552e-03,  2.0866e-04,  3.4752e-04,\n",
       "          5.7543e-04,  2.4874e-04,  1.8193e-03,  3.5986e-04,  5.7625e-04,\n",
       "          3.3108e-03,  2.2478e-04,  1.1284e-03,  1.7503e-04,  3.7151e-04,\n",
       "          3.3051e-04,  1.1757e-04],\n",
       "        [-3.0898e-02,  1.1748e-04,  1.0923e-03,  4.7119e-05,  2.7540e-03,\n",
       "          7.6924e-03,  1.7844e-04,  7.1673e-05,  1.1965e-04,  4.1359e-04,\n",
       "          1.9873e-04,  6.6041e-04,  2.0343e-03,  2.5122e-03,  5.2795e-04,\n",
       "          1.4376e-04,  6.0661e-04,  1.5322e-03,  5.5549e-04,  2.7319e-04,\n",
       "          1.2002e-03,  9.6714e-05,  5.4366e-04,  4.9606e-03,  4.1493e-04,\n",
       "          1.6837e-03,  4.6659e-04],\n",
       "        [ 2.6016e-03,  7.8779e-04,  2.1745e-03,  2.0392e-03,  2.6551e-04,\n",
       "          3.2451e-04,  3.9727e-04,  1.4619e-03, -2.8500e-02,  3.0624e-04,\n",
       "          4.4896e-04,  2.4191e-03,  6.3386e-04,  6.4013e-04,  1.6419e-03,\n",
       "          6.5823e-04,  6.3233e-04,  2.4241e-03,  1.4949e-03,  4.6209e-04,\n",
       "          9.0540e-04,  1.5449e-03,  8.9684e-04,  1.0490e-03,  7.0242e-04,\n",
       "          8.3859e-04,  7.4829e-04],\n",
       "        [ 2.2075e-04,  6.1283e-04,  2.0736e-03,  8.6726e-05,  3.0002e-03,\n",
       "          1.1322e-03,  1.9450e-03,  2.6913e-04,  4.2846e-05,  3.9730e-03,\n",
       "          2.2026e-03,  2.2050e-04,  7.1236e-04,  4.9555e-04,  5.3066e-04,\n",
       "          8.7297e-04,  7.7254e-04,  2.7112e-03,  1.5575e-03,  4.2450e-04,\n",
       "          1.6736e-03,  2.1638e-03,  2.0002e-04,  7.3427e-04,  2.1704e-04,\n",
       "         -3.0299e-02,  1.4540e-03],\n",
       "        [ 1.3569e-03,  8.0162e-04,  4.7181e-04,  2.0668e-03,  1.0229e-03,\n",
       "         -3.1090e-02,  6.8159e-04,  7.5407e-04,  2.2228e-03,  2.6628e-03,\n",
       "          1.6659e-04,  8.0809e-04,  6.4161e-04,  1.3391e-03,  8.7934e-04,\n",
       "          8.2655e-04,  1.8288e-03,  8.5249e-04,  8.8265e-04,  3.2158e-04,\n",
       "          4.5038e-03,  2.5853e-03,  1.3043e-03,  7.5795e-04,  4.0709e-04,\n",
       "          6.2249e-04,  3.2090e-04],\n",
       "        [-2.8479e-02,  6.9657e-04,  6.0163e-04,  1.7351e-03,  1.3942e-03,\n",
       "          1.1595e-03,  9.3095e-04,  5.0723e-04,  2.5629e-03,  1.1855e-03,\n",
       "          8.8363e-04,  4.0412e-04,  1.0923e-03,  8.0171e-04,  2.7098e-04,\n",
       "          5.3980e-04,  2.8691e-04,  9.5071e-04,  1.1984e-03,  3.4493e-03,\n",
       "          1.6943e-03,  8.1776e-05,  1.8813e-03,  2.0216e-03,  1.2972e-03,\n",
       "          4.5681e-04,  3.9468e-04],\n",
       "        [ 1.6715e-03,  5.5445e-04,  6.1310e-04,  8.2419e-04,  1.1662e-03,\n",
       "          7.2976e-04,  6.1814e-04,  7.0071e-04,  1.2897e-03,  2.3972e-03,\n",
       "          8.6852e-04,  9.2258e-05,  5.3513e-04,  3.5634e-03,  3.0578e-04,\n",
       "          7.4202e-04,  3.5062e-04,  9.6941e-04,  3.3429e-03,  2.0670e-03,\n",
       "         -3.0679e-02,  4.0877e-04,  8.0747e-04,  1.7620e-03,  3.4128e-04,\n",
       "          7.4677e-04,  3.2109e-03],\n",
       "        [ 5.6322e-04,  1.3956e-03,  7.8645e-04,  3.0120e-03,  1.3791e-03,\n",
       "          1.2715e-03,  8.6740e-04,  1.4461e-03,  3.7953e-04,  4.1129e-04,\n",
       "          3.4659e-04,  3.4526e-03,  1.3686e-03,  4.1191e-04,  6.9856e-04,\n",
       "          1.9034e-03,  6.2745e-04,  6.9390e-04,  6.6312e-04, -3.0443e-02,\n",
       "          1.1775e-03,  2.8696e-04,  6.5500e-04,  5.5245e-04,  5.4306e-03,\n",
       "          3.0162e-04,  3.6075e-04],\n",
       "        [ 1.8229e-03,  1.2578e-03,  3.4415e-03,  3.5371e-03,  2.3005e-04,\n",
       "          2.4094e-04,  9.8860e-04,  1.1501e-03,  1.1327e-03,  1.5662e-03,\n",
       "          6.1198e-04,  1.3893e-03,  5.5524e-04,  1.6285e-03,  6.0795e-04,\n",
       "         -2.9207e-02,  9.6561e-04,  8.6730e-04,  1.0415e-03,  4.4807e-04,\n",
       "          6.4711e-04,  7.2161e-04,  1.3498e-03,  6.6375e-04,  3.3418e-04,\n",
       "          1.3556e-03,  6.5148e-04],\n",
       "        [ 5.4623e-04,  1.0535e-03,  2.0311e-04,  1.4699e-03,  3.3909e-03,\n",
       "          5.1061e-04,  3.2123e-04,  1.9676e-03,  8.4854e-04,  1.8654e-03,\n",
       "          7.1777e-04,  2.5755e-04, -3.1041e-02,  1.6838e-03,  2.2936e-04,\n",
       "          1.0211e-03,  1.1475e-03,  1.1906e-04,  5.9962e-04,  6.1527e-03,\n",
       "          6.5609e-04,  3.0206e-04,  5.7503e-04,  1.1636e-03,  3.2353e-03,\n",
       "          4.8693e-04,  5.1698e-04],\n",
       "        [ 1.9351e-03,  8.8727e-04,  2.5451e-03,  2.1258e-03,  1.1344e-04,\n",
       "          1.5384e-04,  1.3921e-03,  1.0446e-03,  1.6019e-03,  8.5878e-04,\n",
       "          9.6111e-04,  1.5213e-03,  9.0171e-04,  1.1535e-03,  1.5292e-03,\n",
       "          1.0088e-03,  4.4198e-04,  2.4740e-03,  9.6268e-04,  4.3262e-04,\n",
       "          4.3442e-04,  8.2145e-04, -3.0090e-02,  1.9986e-03,  1.3177e-03,\n",
       "          1.0394e-03,  4.3322e-04],\n",
       "        [ 3.0241e-03,  1.9051e-03,  1.3208e-03,  2.6593e-03,  2.6103e-04,\n",
       "          3.4904e-04,  1.7333e-03,  5.2497e-04,  2.1896e-03,  4.4859e-04,\n",
       "          7.5326e-05,  1.6768e-03,  1.3384e-03,  1.0494e-03,  9.7314e-04,\n",
       "          3.9873e-04,  5.4926e-04,  1.5190e-03,  1.5941e-03,  5.2119e-04,\n",
       "          6.5805e-04,  1.6601e-04, -3.1013e-02,  7.5132e-04,  3.5377e-03,\n",
       "          3.8538e-04,  1.4030e-03],\n",
       "        [ 2.6016e-03,  7.8779e-04, -2.9075e-02,  2.0392e-03,  2.6551e-04,\n",
       "          3.2451e-04,  3.9727e-04,  1.4619e-03,  2.7503e-03,  3.0624e-04,\n",
       "          4.4896e-04,  2.4191e-03,  6.3386e-04,  6.4013e-04,  1.6419e-03,\n",
       "          6.5823e-04,  6.3233e-04,  2.4241e-03,  1.4949e-03,  4.6209e-04,\n",
       "          9.0540e-04,  1.5449e-03,  8.9684e-04,  1.0490e-03,  7.0242e-04,\n",
       "          8.3859e-04,  7.4829e-04],\n",
       "        [ 2.1327e-03,  7.4616e-04,  1.2464e-03,  2.7629e-03,  5.1268e-04,\n",
       "          2.0048e-04,  6.8627e-04,  1.2203e-03,  3.8831e-03,  1.1437e-03,\n",
       "          2.9368e-04,  2.5077e-03,  2.3969e-04,  5.5699e-04,  1.3108e-03,\n",
       "          1.5637e-03,  7.4265e-04,  1.4185e-03,  1.3852e-03,  2.7886e-04,\n",
       "          1.7187e-03, -2.9279e-02,  3.5207e-04,  3.1881e-04,  6.2191e-04,\n",
       "          1.1474e-03,  2.8811e-04],\n",
       "        [ 2.6016e-03,  7.8779e-04,  2.1745e-03,  2.0392e-03,  2.6551e-04,\n",
       "          3.2451e-04,  3.9727e-04,  1.4619e-03,  2.7503e-03,  3.0624e-04,\n",
       "          4.4896e-04,  2.4191e-03,  6.3386e-04,  6.4013e-04,  1.6419e-03,\n",
       "          6.5823e-04,  6.3233e-04,  2.4241e-03, -2.9755e-02,  4.6209e-04,\n",
       "          9.0540e-04,  1.5449e-03,  8.9684e-04,  1.0490e-03,  7.0242e-04,\n",
       "          8.3859e-04,  7.4829e-04]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4519be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = probs.clone()\n",
    "dlogits[range(probs.shape[0]), Yb] -= 1.0\n",
    "# for i in range(probs.shape[0]):\n",
    "#     dlogits[i][Yb[i]] -= 1.0\n",
    "dlogits /= probs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0363b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: False | approximate: True  | maxdiff: 5.122274160385132e-09\n"
     ]
    }
   ],
   "source": [
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5da65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnmeani = (1 / n) * (hprebn.sum(0, keepdim = True))\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff ** 2\n",
    "# bnvar = (1 / (n - 1)) * (bndiff2.sum(0, keepdim = True))\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "\n",
    "dhprebn = (1.0 - 1.0 / h.shape[1]) * (h.shape[0] - 1) * bnraw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1028ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: False | maxdiff: 0.3773874044418335\n"
     ]
    }
   ],
   "source": [
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad35e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: False | maxdiff: 0.002149573527276516\n"
     ]
    }
   ],
   "source": [
    "dhprebn = ((1.0 - 1.0 / vocab_size) * bnvar_inv) * bngain * hpreact.grad \n",
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e64a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = h.shape[0]\n",
    "n = h.shape[1]\n",
    "dhprebn = ((bngain * bnvar_inv) / m) * (m * dhpreact - dhpreact.sum(0) - (m/(m-1)) * bnraw * (dhpreact * bnraw).sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b59842e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e247a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
